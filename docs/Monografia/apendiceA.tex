\chapter{Filtro de Kalman Estendido} \label{sec:apendicea}

A fim de estimar o posicionamento do robô a partir de diversos sensores, uma técnica frequentemente utilizada é o filtro de Kalman (KF). Este filtro opera de forma a minimizar o erro entre a estimativa de um estado e o estado real do processo por meio de duas técnicas: o método dos mínimos quadrados e a propagação de estados e covariâncias.

Nesta seção, é descrito o método dos mínimos quadrados e a propagação de estado e sua covariância no tempo. Em seguida, descreve-se o filtro de Kalman discreto para sistemas lineares e, finalmente, o filtro de Kalman estendido (EKF), utilizado neste trabalho. O desenvolvimento deste capítulo foi baseado no trabalho de \cite{Simon06OptimalState}.

\section{Método dos mínimos quadrados}

Suponha-se um conjunto de medições sobre uma determinada constante, adquiridas com um equipamento ruidoso. Cada medida é adquirida de forma independente e possui variâncias distintas.
Pode-se modelar o problema como:
\begin{equation}
	\textbf{y} = H\textbf{x} + \textbf{v}
\end{equation}
Onde $\textbf{y}$ é um vetor com as medições, $\textbf{x}$ é a constante a ser determinada, mas que também se apresenta em forma de vetor (iguais) e $\textbf{v}$ é o ruído da medição.

Suponha-se também que as medições são independentes e com média nula. A matriz da covariância fica então com a seguinte forma:
\begin{equation}
  R = 
	\left[
  \begin{array}{ccc}
	  \sigma^{2}_{1} & \cdots & 0 \\
	  \vdots & \ddots & \vdots \\
	  0 & \cdots & \sigma^{2}_{k} \\
	\end{array}
	\right]
\end{equation}
Considerando-se que o erro de cada medida é, em forma vetorial
\begin{equation}
	\epsilon_{y}=\textbf{y}-H\widehat{\textbf{x}}
\end{equation}
o método dos mínimos quadrados consiste na minimização da seguinte função de custo:
\begin{equation}
	J=\frac{\epsilon^{2}_{y1}}{\sigma^{2}_{1}}+\cdots+\frac{\epsilon^{2}_{yk}}{\sigma^{2}_{k}}
	\label{eq:funcaodecusto}
\end{equation}
Este é o método dos mínimos quadrados aplicado quando cada medição possui variância distinta. Sua principal característica é ser a forma ótima de minimizar os erros provenientes de medições imprecisas.

O problema com a abordagem acima é que todas as vezes que uma nova estimativa é acrescida ao sistema é necessário efetuar a multiplicação de $H$ por $\textbf{x}$ e minimizar $J$ novamente. Para poucas medições isto é pouco oneroso, mas para o caso de muitas medidas, e.g., 10000 medições, a adição de uma medida implica na multiplicação de uma matriz 10001xN por outra Nx1, não aproveitando-se a estimativa obtida com as medições antigas. O método dos mínimos quadrados recursivo resolve esta questão, e é a forma utilizada para atualização do estado no filtro de kalman.

\section{Método dos mínimos quadrados recursivo}
Um estimador linear recursivo pode ser escrito na forma:
\begin{gather}
	\textbf{y}_{k} = H_{k}\textbf{x}+\textbf{v}_{k} \nonumber \\
	\hat{\textbf{x}}_{k} = \hat{\textbf{x}}_{k-1} + K_{k}(\textbf{y}_{k}-H_{k}\hat{\textbf{x}}_{k-1})
\end{gather}
$K_{k}$ é uma matriz a ser determinada, chamada na literatura de matriz de ganho do estimador. O termo $(\textbf{y}_{k}-H_{k}\hat{\textbf{x}}_{k-1})$ é chamado de termo de correção. Se a matriz de ganho ou termo de correção forem nulos a estimativa do estado não muda.

Calculando o erro da da estimativa, temos que:
\begin{align}
	E(\epsilon_{x,k})& = E(\textbf{x}-\hat{\textbf{x}}_{k}) \nonumber \\
	                 & = E[\text{x}-\hat{\textbf{x}}_{k-1}-K_{k}(\textbf{y}_{k}-H_{k}\hat{\textbf{x}}_{k-1})] \nonumber \\
	                 & = E[\epsilon_{x,k-1}-K_{k}(H_{k}\textbf{x}+\textbf{v}_{k}-H_{k}\hat{\textbf{x}}_{k-1})] \nonumber \\
	                 & = E[\epsilon_{x,k-1}-K_{k}H_{k}(\textbf{x}-\hat{\textbf{x}}_{k-1})-K_{k}\textbf{v}_{k}] \nonumber \\
	                 & = (I - K_{k}H_{k})E(\epsilon_{\textbf{x},k-1})-K_{k}E(\textbf{v}_{k}) \label{eq:erroestimativarecursivo}
\end{align}
Verifica-se que $\epsilon_{x,k} = 0$ se $E(\epsilon_{x,k-1}) = 0$ e $E(\textbf{v}_{k}) = 0$. É importante salientar que isto é válido para qualquer $K_{k}$, o que fornece liberdade na hora de calcular $K_{k}$.

Para calcular a matriz $K_{k}$ de maneira ótima, o método utilizado é o da minimização da soma das variâncias no tempo $k$. Assim, de forma análoga à \ref{eq:funcaodecusto}:
\begin{align}
	J_{k} & = E[(x_{1} - \hat{x}_{1})^{2}]+\cdots + E[(x_{n} - \hat{x}_{n})^{2}] \nonumber \\
		  & = E(\epsilon_{x1,k}^{2}+\cdots + \epsilon_{xn,k}^{2}) \nonumber \\
		  & = E(\epsilon_{x,k}^{T}\epsilon_{x,k}) \nonumber \\
		  & = E[Tr(\epsilon_{x,k}^{T}\epsilon_{x,k}] \nonumber \\
		  & = TrP_{k} \label{eq:custorecursivo}
\end{align}
Substituindo \ref{eq:erroestimativarecursivo} em \ref{eq:custorecursivo}, obtem-se
\[
	P_{k} = (I-K_{k}H_{k})P_{k-1}(I-K_{k}H_{k})^{T} + K_{k}R_{k}K_{k}^{T}
	\label{eq:calculocovariancia}
\]
onde $R_{k} = E(\textbf{v}_{k}\textbf{v}_{k}^{T})$.

Tomando-se $\frac{\partial J_{k}}{\partial K_{k}} = 0$ obtemos:
\[
	K_{k} = P_{k-1}H_{k}^{T}(H_{k}P_{k-1}H_{k}^{T} + R_{k})^{-1}
	\label{eq:calculok}
\]
Assim, o método dos mínimos quadrados recursivo é obtido através das seguintes equações:
\begin{align}
	K_{k} 		& = P_{k-1}H_{k}^{T}(H_{k}P_{k-1}H_{k}^{T} + R_{k})^{-1} \nonumber \\
	\hat{\textbf{x}}_{k} & = \hat{\textbf{x}}_{k-1} + K_{k}(\textbf{y}_{k}-H_{k}\hat{\textbf{x}}_{k-1}) \nonumber \\
	P_{k} 		& = (I-K_{k}H_{k})P_{k-1}(I-K_{k}H_{k})^{T} + K_{k}R_{k}K_{k}^{T}
\end{align}

\section{Propagação de estados e covariâncias}
Para se obter uma estimativa correta do estado e da covariância após um determinado intervalo de tempo decorrido, suponha-se o seguinte sistema diferencial linear genérico:
\[
	\dot{\textbf{x}}=A\textbf{x}+B\textbf{u}+\textbf{w}
\]
Onde $\textbf{x}$ é o estado do sistema, $\textbf{u}$ é uma variável de controle e $\textbf{w}$ é o ruído do sistema, com média nula e com distribuição gaussiana. Por ser uma equação diferencial, sabemos que a solução do sistema acima para um determinado tempo $t_{k}$ é:
\begin{equation}
	\textbf{x}(t_{k})=e^{A(t_{k}-t_{k-1})}\textbf{x}(t_{k-1})+\int^{t_{k}}_{t_{k-1}}e^{A(t_{k}-\tau)}[B(\tau)\textbf{u}(\tau)+\textbf{w}(\tau)]d\tau
	\label{eq:solDiferencial}
\end{equation}
Agora fazendo-se a seguinte hipótese
\[
	\textbf{u}(t)=\textbf{u}_{k}, t \in [t_{k-1},t_{k}]
\]
e as seguintes definições
\begin{gather*}
	\Delta t=t_{k}-t_{k-1} \\
	\textbf{x}_{k}=\textbf{x}(t_{k}) \\
	\textbf{u}_{k}=\textbf{u}(t_{k}) \\
	F_{k}=e^{A\Delta t} \\
	G_{k}=\int^{t_{k+1}}_{t_{k}}e^{A(t_{k+1}-\tau)}B(\tau)d\tau
\end{gather*}
A equação \eqref{eq:solDiferencial} é reescrita como:
\[	\textbf{x}_{k}=F_{k-1}\textbf{x}_{k-1}+G_{k-1}\textbf{u}_{k-1}+\int^{t_{k}}_{t_{k-1}}e^{A(t_{k}-\tau)}\textbf{w}(\tau)d\tau
\]
Obtendo-se a média da equação acima, e lembrando-se que o ruído possui média nula, obtemos finalmente
\begin{equation}
	\bar{\textbf{x}}_{k}=F_{k-1}\bar{\textbf{x}}_{k-1}+G_{k-1}\textbf{u}_{k-1}
	\label{eq:estimativaX}
\end{equation}
A propagação da matriz de covariâncias é dada pela seguinte fórmula:
\[
	P_{k}=E[(\textbf{x}_{k}-\bar{\textbf{x}}_{k})(\textbf{x}_{k}-\bar{\textbf{x}}_{k})^{T}]
\]
Utilizando-se a equação \eqref{eq:estimativaX} e assumindo como hipótese que $w(t)$ é um ruído branco de covariância $Q_{c}(t)$ obtém-se, após alguma manipulação matemática, a sequinte equação para $P_{k}$:
\begin{equation}
	P_{k}=F_{k-1}P_{k-1}F^{T}_{k-1}+Q_{k-1}
\end{equation}
Onde $Q_{k-1}$ pode ser aproximado por
\[
	Q_{k-1}\approx Q_{c}(t_{k})\Delta t
\]
e $Q_{c}$ é a covariância do ruído branco $w(t)$.

\section{Filtro de Kalman discreto}
De posse da base teórica acima pode-se descrever o filtro de Kalman, que opera propagando o estado e a covariância no tempo e atualizando-os com os valores medidos, usando-se para isso o método dos mínimos quadrados recursivo. Em suma:
\begin{itemize}
	\item Uma descrição matemárica do problema para os quais a estimação dos estados são necessárias é realizada.
	\item Propaga-se o estado e a covariância no tempo como descrito anteriormente.
	\item Atualiza-se o sistema com informação vinda de observações quando estas são disponíveis.
\end{itemize}
Assim, o sistema é modelado da seguinte forma
\begin{gather}
	\textbf{x}_{t}=F_{t-1}\textbf{x}_{t-1}+B_{t-1}\textbf{u}_{t-1}+\textbf{w}_{t-1} \nonumber \\
	\textbf{y}_{t}=H_{t}\textbf{x}_{t}+\textbf{v}_{t}
\end{gather}
onde os ruídos $\textbf{w}_{t-1}$ e $\textbf{v}_{t}$ são brancos, centrados no zero, não correlatos e possuem matrizes de covariância $Q$ e $R$ respectivamente.

Para estimar o valor de $\textbf{x}_{t}$ divide-se $\textbf{x}_{t}$ em dois valores distintos, $\hat{\textbf{x}}_{t}^{-}$ e $\hat{\textbf{x}}_{t}^{+}$, onde o primeiro corresponde a um valor antes de uma observação e o segundo após a observação, i.e., $\hat{\textbf{x}}_{t}^{-}$ corresponde ao valor estimado baseado na dinâmica do sistema em um instante $t$ instantâneamente antes de uma medição ser realizada, enquanto que $\hat{\textbf{x}}_{t}^{+}$ corresponde ao valor estimado no mesmo instante $t$ após a obtenção de uma estimativa $\textbf{y}_{t}$. De forma semelhante, $P_{t}$ é dividido em $P_{t}^{-}$ e $P_{t}^{+}$, sendo estas as matrizes de covariâncias antes e depois de uma observação no instante $t$ respectivamente.

Tendo definido estes estados e de posse da teoria previamente discutida, pode-se descrever o processo de estimação através do filtro de Kalman com o seguinte processo:
\begin{itemize}
	\item Descreve-se matematicamente o sistema:
		\begin{gather}
			\textbf{x}_{t}=F_{t-1}\textbf{x}_{t-1}+B_{t-1}\textbf{u}_{t-1}+\textbf{w}_{t-1} \nonumber \\
			\textbf{y}_{t}=H_{t}\textbf{x}_{t}+\textbf{v}_{t} \nonumber \\
			E(\textbf{w}_{t}\textbf{w}^{T}_{j})=Q_{t}\delta_{t-j} \\
			E(\textbf{v}_{t}\textbf{v}^{T}_{j})=R_{t}\delta_{t-j} \nonumber \\
			E(\textbf{w}_{t}\textbf{v}^{T}_{j})=0 \nonumber 
		\end{gather}	
	\item Inicia-se o filtro:
		\begin{gather}
			\hat{\textbf{x}}^{+}_{0}=E(\textbf{x}_{0}) \nonumber \\
			P^{+}_{0}=E[(\textbf{x}_{0}-\hat{\textbf{x}}^{+}_{0})(\textbf{x}_{0}-\hat{\textbf{x}}^{+}_{0})^{T}]
		\end{gather}
	\item Itera-se o filtro para cada uma das seguintes equações:
		\begin{gather}
			P^{-}_{t}=F_{t-1}P^{+}_{t-1}F^{T}_{t-1}+Q_{t-1} \nonumber \\
			\bar{\textbf{x}}^{-}_{t}=F_{t-1}\hat{\textbf{x}}^{+}_{t-1}+B_{t-1}\textbf{u}_{t-1} \nonumber \\
			K_{k}=P^{-}_{t}H^{T}_{t}(H_{t}P^{-}_{t}H^{T}_{t}+R_{t})^{-1} \\
			\hat{\textbf{x}}^{+}_{t}=\bar{\textbf{x}}^{-}_{k}+K_{t}(\textbf{y}_{t}-H_{t}\bar{\textbf{x}}^{-}_{t}) \nonumber \\
			P^{+}_{t}=(I-K_{t}H_{t})P^{-}_{t} \nonumber
		\end{gather}
\end{itemize}
Ou seja, o sistema é suprido de uma estimativa inicial do estado e da covariância da estimativa. Se o estado é conhecido, $P_{0}^{+}=0$. A partir daí, propaga-se o sistema no tempo até o momento da primeira medição, obtendo-se $P_{1}^{-}$ e $\hat{\textbf{x}}_{1}^{-}$. Após isto, o valor da estimativa $\textbf{y}_{1}$ é utilizado para atualizar a estimativa e covariância, obtendo-se assim $\hat{\textbf{x}}_{1}^{+}$ e $P_{1}^{+}$.

\section{Filtro de Kalman Estendido}
O filtro de Kalman desenvolvido anteriormente é matematicamente o método ótimo de estimação de estados para sistemas lineares. No entanto, na natureza existem poucos sistemas lineares, a maioria são sistemas lineares, e o problema de estimação da posição do robô com sonares e visão computacional não é linear. Assim, é necessário um outro filtro para isso.

Foi desenvolvido uma versão extendida para estimação de sistemas não lineares, usado inicialmente para o sistema de navegação de espaçonaves, e atualmente é o filtro padrão para estimação de sistemas não lineares.

No filtro estendido, os estados de transição e modelos de observação não precisam ser funções lineares, podendo ser equações diferenciais. Em nossa formulação, usaremos o método hibrido de filtragem, que consiste em propagação de estado de forma contínua e atualização de estado com medidas de forma discreta. Isto porque nosso poder computacional nos permite propagar o estado segundo nosso modelo de dinâmica várias vezes entre duas medidas de sonares estarem disponíveis.

No EKF, o sistema é modelado da seguinte forma:
\begin{gather}
	\dot{\textbf{x}}	= f(\textbf{x},\textbf{u},\textbf{w},t) \nonumber \\
	\textbf{y}			= h(\textbf{x},\textbf{v},t)
\end{gather}
Onde $\textbf{x}$ é o estado, $\textbf{u}$ é uma força de controle, $\textbf{w}$ o ruído do processo e $\textbf{v}$ o ruído da medição.

Estas equações bastam para predizer a estimativa e atualizá-la com a nova medida. No entanto, não há como diretamente aplicá-las na obtenção da covariância. Para obter a covariância, é realizada uma expansão de taylor de primeira ordem ao redor de cada estimativa, linearizando o sistema segundo uma função normalizada, e após isto o filtro de Kalman discreto anteriormente descrito é utilizado no sistema agora linear.

Para o processo de linearização do entorno da estimativa, é necessário o cálculo do jacobiano de $f$ e $h$, obtendo-se $F_{k-1}$ e $H_{k}$ respectivamente, que serão utilizados nas equações de filtro linear, a saber:
\begin{gather}
	P^{-}_{k}=F_{t-1}P^{+}_{t-1}F^{T}_{t-1}+Q_{t-1} \nonumber \\
	K_{t}=P^{-}_{k}H^{T}_{t}(H_{t}P^{-}_{t}H^{T}_{k}+R_{t})^{-1} \\
	P^{+}_{t}=(I-K_{t}H_{t})P^{-}_{t} \nonumber
\end{gather}
É importante notar que para sistemas com não-linearidades muito grandes a expansão apresenta erros muito grandes. No entanto, para o problema de estimativa de localização de um robô o EKF apresenta performance satisfatória.